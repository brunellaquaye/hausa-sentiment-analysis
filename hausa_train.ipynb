{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0cdd6f",
   "metadata": {},
   "source": [
    "# Hausa Sentiment Analysis: Model Training\n",
    "\n",
    "This notebook demonstrates how to fine-tune the HausaBERTa transformer model on the preprocessed Hausa sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e6e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample preprocessed train text:\n",
      "['kudin arewa babu abin azo agani alummah allah ya isa yacucemu wlh yarikitamana kasa yarikitamana kasuwanci harkar ilimi harkar lfy hanyoyi babu lantarki dasuransu komai yalalace cinhanci rashawa fili nigeria jamiyaryar tabataman mlm', 'kaga adu ar banda wai haka shugaban sojoji gaskiya buhari kaci amanan kasa mutum ah wajen nan', 'haquri yan madrid daman kunce champion din ya muku yawa', 'hmm kasan girman allah daxakace mukuma allah kune kukabarshi kuna karyata ayoyinsa kace allah baya karbar adduar talakan nigeria kunzalunceshi allah ya karbar adduar aka zalunta sauri kace wai allah baya karbar addua talakawa', 'wai gwamno nin nigeria suna afa kwayoyi']\n",
      "Sample train labels:\n",
      "[2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Load Preprocessed Cleaned Data\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('data/afrisenti_twitter_hausa_train_clean.csv')\n",
    "val_df = pd.read_csv('data/afrisenti_twitter_hausa_validation_clean.csv')\n",
    "test_df = pd.read_csv('data/afrisenti_twitter_hausa_test_clean.csv')\n",
    "\n",
    "train_texts = train_df['tweet_clean'].tolist()\n",
    "val_texts = val_df['tweet_clean'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "val_labels = val_df['label'].tolist()\n",
    "\n",
    "print('Sample preprocessed train text:')\n",
    "print(train_texts[:5])\n",
    "print('Sample train labels:')\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04cc723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training set size: 14172\n",
      "Validation set size: 2677\n",
      "Preprocessing text...\n",
      "\n",
      "Label distribution in training:\n",
      "label\n",
      "0    4687\n",
      "1    4912\n",
      "2    4573\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training and comparing multiple models...\n",
      "Model                Accuracy   F1-Macro   F1-Weighted \n",
      "-------------------------------------------------------\n",
      "\n",
      "Label distribution in training:\n",
      "label\n",
      "0    4687\n",
      "1    4912\n",
      "2    4573\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training and comparing multiple models...\n",
      "Model                Accuracy   F1-Macro   F1-Weighted \n",
      "-------------------------------------------------------\n",
      "logistic_enhanced    0.7396     0.7416     0.7413      \n",
      "logistic_enhanced    0.7396     0.7416     0.7413      \n",
      "char_ngrams          0.7561     0.7570     0.7568      \n",
      "char_ngrams          0.7561     0.7570     0.7568      \n",
      "feature_union        0.7135     0.7143     0.7141      \n",
      "feature_union        0.7135     0.7143     0.7141      \n",
      "svm_rbf              0.7408     0.7425     0.7423      \n",
      "svm_rbf              0.7408     0.7425     0.7423      \n",
      "complement_nb        0.7030     0.7016     0.7013      \n",
      "\n",
      "Best model: char_ngrams (F1-Macro: 0.7570)\n",
      "\n",
      "Performing hyperparameter tuning on char_ngrams...\n",
      "\n",
      "Creating ensemble from top performing models...\n",
      "Training ensemble...\n",
      "complement_nb        0.7030     0.7016     0.7013      \n",
      "\n",
      "Best model: char_ngrams (F1-Macro: 0.7570)\n",
      "\n",
      "Performing hyperparameter tuning on char_ngrams...\n",
      "\n",
      "Creating ensemble from top performing models...\n",
      "Training ensemble...\n",
      "Ensemble Accuracy: 0.7572\n",
      "Ensemble F1-Macro: 0.7586\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Best model: ensemble\n",
      "Best validation accuracy: 0.7572\n",
      "Best validation F1-macro: 0.7586\n",
      "\n",
      "Classification report for ensemble:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       887\n",
      "           1       0.68      0.75      0.71       896\n",
      "           2       0.74      0.70      0.72       894\n",
      "\n",
      "    accuracy                           0.76      2677\n",
      "   macro avg       0.76      0.76      0.76      2677\n",
      "weighted avg       0.76      0.76      0.76      2677\n",
      "\n",
      "\n",
      "Saving models...\n",
      "Ensemble Accuracy: 0.7572\n",
      "Ensemble F1-Macro: 0.7586\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Best model: ensemble\n",
      "Best validation accuracy: 0.7572\n",
      "Best validation F1-macro: 0.7586\n",
      "\n",
      "Classification report for ensemble:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       887\n",
      "           1       0.68      0.75      0.71       896\n",
      "           2       0.74      0.70      0.72       894\n",
      "\n",
      "    accuracy                           0.76      2677\n",
      "   macro avg       0.76      0.76      0.76      2677\n",
      "weighted avg       0.76      0.76      0.76      2677\n",
      "\n",
      "\n",
      "Saving models...\n",
      "Models and results saved successfully!\n",
      "Best model saved as: models/hausa_sentiment/best_model.joblib\n",
      "Compatibility model saved as: models/hausa_sentiment/logreg_model.joblib\n",
      "Models and results saved successfully!\n",
      "Best model saved as: models/hausa_sentiment/best_model.joblib\n",
      "Compatibility model saved as: models/hausa_sentiment/logreg_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('.')\n",
    "from hausa_preprocess import HausaTextPreprocessor\n",
    "\n",
    "class HausaFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom feature extractor for Hausa text that integrates with sklearn pipelines.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.preprocessor = HausaTextPreprocessor()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for text in X:\n",
    "            _, feature_dict = self.preprocessor.preprocess(str(text), extract_features=True)\n",
    "            features.append(list(feature_dict.values()))\n",
    "        return np.array(features)\n",
    "\n",
    "# Load cleaned Hausa sentiment data\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv('data/afrisenti_twitter_hausa_train_clean.csv')\n",
    "val_df = pd.read_csv('data/afrisenti_twitter_hausa_validation_clean.csv')\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Use robust Hausa preprocessor\n",
    "print(\"Preprocessing text...\")\n",
    "preprocessor = HausaTextPreprocessor()\n",
    "train_df['tweet_clean'] = train_df['tweet_clean'].astype(str).apply(preprocessor.preprocess)\n",
    "val_df['tweet_clean'] = val_df['tweet_clean'].astype(str).apply(preprocessor.preprocess)\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_df['tweet_clean']\n",
    "y_train = train_df['label']\n",
    "X_val = val_df['tweet_clean']\n",
    "y_val = val_df['label']\n",
    "\n",
    "# Enhanced: Show label distribution\n",
    "print(f\"\\nLabel distribution in training:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "# Encode labels if not already numeric\n",
    "if y_train.dtype == object:\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_val = label_encoder.transform(y_val)\n",
    "    print(f\"Labels encoded. Classes: {label_encoder.classes_}\")\n",
    "else:\n",
    "    label_encoder = None\n",
    "\n",
    "# Enhanced: Create multiple model configurations for comparison\n",
    "print(\"\\nTraining and comparing multiple models...\")\n",
    "\n",
    "models_to_test = {}\n",
    "\n",
    "# 1. Enhanced Logistic Regression (your original, but improved)\n",
    "models_to_test['logistic_enhanced'] = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=15000,      # Increased vocabulary\n",
    "        ngram_range=(1, 3),      # Include trigrams\n",
    "        min_df=2,               # Remove very rare words\n",
    "        max_df=0.95,            # Remove very common words  \n",
    "        sublinear_tf=True       # Better for large feature spaces\n",
    "    )),\n",
    "    ('clf', LogisticRegression(\n",
    "        max_iter=2000, \n",
    "        class_weight='balanced', \n",
    "        C=1.0,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. Character n-grams for morphological features\n",
    "models_to_test['char_ngrams'] = Pipeline([\n",
    "    ('tfidf_char', TfidfVectorizer(\n",
    "        analyzer='char_wb',\n",
    "        ngram_range=(3, 6),\n",
    "        max_features=10000,\n",
    "        min_df=2\n",
    "    )),\n",
    "    ('clf', LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight='balanced',\n",
    "        C=10.0,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 3. Feature union combining different representations\n",
    "models_to_test['feature_union'] = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('word_tfidf', TfidfVectorizer(\n",
    "            max_features=10000, \n",
    "            ngram_range=(1,2),\n",
    "            min_df=2\n",
    "        )),\n",
    "        ('char_tfidf', TfidfVectorizer(\n",
    "            analyzer='char_wb',\n",
    "            ngram_range=(3,5),\n",
    "            max_features=5000,\n",
    "            min_df=2\n",
    "        )),\n",
    "        ('linguistic', HausaFeatureExtractor())\n",
    "    ])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),  # For mixed features\n",
    "    ('clf', LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 4. Support Vector Machine with RBF kernel\n",
    "models_to_test['svm_rbf'] = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=12000,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2\n",
    "    )),\n",
    "    ('clf', SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5. Complement Naive Bayes (good for imbalanced text data)\n",
    "models_to_test['complement_nb'] = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=12000,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2\n",
    "    )),\n",
    "    ('clf', ComplementNB(alpha=0.1))\n",
    "])\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {}\n",
    "best_model = None\n",
    "best_f1 = 0\n",
    "\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'F1-Macro':<10} {'F1-Weighted':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for model_name, pipeline in models_to_test.items():\n",
    "    try:\n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_preds = pipeline.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, val_preds)\n",
    "        val_f1_macro = f1_score(y_val, val_preds, average='macro')\n",
    "        val_f1_weighted = f1_score(y_val, val_preds, average='weighted')\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'pipeline': pipeline,\n",
    "            'accuracy': val_acc,\n",
    "            'f1_macro': val_f1_macro,\n",
    "            'f1_weighted': val_f1_weighted,\n",
    "            'predictions': val_preds\n",
    "        }\n",
    "        \n",
    "        print(f\"{model_name:<20} {val_acc:<10.4f} {val_f1_macro:<10.4f} {val_f1_weighted:<12.4f}\")\n",
    "        \n",
    "        # Track best model\n",
    "        if val_f1_macro > best_f1:\n",
    "            best_f1 = val_f1_macro\n",
    "            best_model = model_name\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {e}\")\n",
    "\n",
    "print(f\"\\nBest model: {best_model} (F1-Macro: {best_f1:.4f})\")\n",
    "\n",
    "# Enhanced: Hyperparameter tuning on best model\n",
    "if best_model and best_model != 'feature_union':  # Skip complex model for quick tuning\n",
    "    print(f\"\\nPerforming hyperparameter tuning on {best_model}...\")\n",
    "    \n",
    "    base_pipeline = results[best_model]['pipeline']\n",
    "    \n",
    "    # Define parameter grid based on model type\n",
    "    if 'logistic' in best_model:\n",
    "        param_grid = {\n",
    "            'tfidf__max_features': [12000, 15000, 20000],\n",
    "            'clf__C': [0.1, 1.0, 10.0, 100.0],\n",
    "            'clf__penalty': ['l1', 'l2'],\n",
    "            'clf__solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    elif 'svm' in best_model:\n",
    "        param_grid = {\n",
    "            'tfidf__max_features': [10000, 15000],\n",
    "            'clf__C': [0.1, 1.0, 10.0],\n",
    "            'clf__gamma': ['scale', 'auto']\n",
    "        }\n",
    "    else:\n",
    "        param_grid = None\n",
    "    \n",
    "    if param_grid:\n",
    "        # Perform grid search\n",
    "        cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            base_pipeline,\n",
    "            param_grid,\n",
    "            cv=cv_strategy,\n",
    "            scoring='f1_macro',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate tuned model\n",
    "        tuned_preds = grid_search.best_estimator_.predict(X_val)\n",
    "        tuned_acc = accuracy_score(y_val, tuned_preds)\n",
    "        tuned_f1 = f1_score(y_val, tuned_preds, average='macro')\n",
    "        \n",
    "        print(f\"Tuned model results:\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Validation Accuracy: {tuned_acc:.4f}\")\n",
    "        print(f\"Validation F1-Macro: {tuned_f1:.4f}\")\n",
    "        \n",
    "        # Update best model if tuning improved performance\n",
    "        if tuned_f1 > best_f1:\n",
    "            results[f'{best_model}_tuned'] = {\n",
    "                'pipeline': grid_search.best_estimator_,\n",
    "                'accuracy': tuned_acc,\n",
    "                'f1_macro': tuned_f1,\n",
    "                'f1_weighted': f1_score(y_val, tuned_preds, average='weighted'),\n",
    "                'predictions': tuned_preds\n",
    "            }\n",
    "            best_model = f'{best_model}_tuned'\n",
    "            best_f1 = tuned_f1\n",
    "\n",
    "# Enhanced: Create ensemble of top models\n",
    "print(f\"\\nCreating ensemble from top performing models...\")\n",
    "top_models = sorted(results.items(), key=lambda x: x[1]['f1_macro'], reverse=True)[:3]\n",
    "\n",
    "ensemble_models = []\n",
    "for name, result in top_models:\n",
    "    if hasattr(result['pipeline'].named_steps['clf'], 'predict_proba'):\n",
    "        ensemble_models.append((name, result['pipeline']))\n",
    "\n",
    "if len(ensemble_models) >= 2:\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=ensemble_models,\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"Training ensemble...\")\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    \n",
    "    ensemble_preds = voting_clf.predict(X_val)\n",
    "    ensemble_acc = accuracy_score(y_val, ensemble_preds)\n",
    "    ensemble_f1 = f1_score(y_val, ensemble_preds, average='macro')\n",
    "    \n",
    "    results['ensemble'] = {\n",
    "        'pipeline': voting_clf,\n",
    "        'accuracy': ensemble_acc,\n",
    "        'f1_macro': ensemble_f1,\n",
    "        'f1_weighted': f1_score(y_val, ensemble_preds, average='weighted'),\n",
    "        'predictions': ensemble_preds\n",
    "    }\n",
    "    \n",
    "    print(f\"Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
    "    print(f\"Ensemble F1-Macro: {ensemble_f1:.4f}\")\n",
    "    \n",
    "    if ensemble_f1 > best_f1:\n",
    "        best_model = 'ensemble'\n",
    "        best_f1 = ensemble_f1\n",
    "\n",
    "# Enhanced: Display final results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best model: {best_model}\")\n",
    "print(f\"Best validation accuracy: {results[best_model]['accuracy']:.4f}\")\n",
    "print(f\"Best validation F1-macro: {results[best_model]['f1_macro']:.4f}\")\n",
    "\n",
    "# Show detailed classification report for best model\n",
    "print(f\"\\nClassification report for {best_model}:\")\n",
    "best_preds = results[best_model]['predictions']\n",
    "if label_encoder:\n",
    "    target_names = label_encoder.classes_\n",
    "else:\n",
    "    target_names = None\n",
    "print(classification_report(y_val, best_preds, target_names=target_names))\n",
    "\n",
    "# Enhanced: Save best model with metadata\n",
    "print(f\"\\nSaving models...\")\n",
    "os.makedirs('models/hausa_sentiment', exist_ok=True)\n",
    "\n",
    "# Save the best performing model\n",
    "best_pipeline = results[best_model]['pipeline']\n",
    "joblib.dump(best_pipeline, 'models/hausa_sentiment/best_model.joblib')\n",
    "\n",
    "# Save original logistic regression model for compatibility\n",
    "if 'logistic_enhanced' in results:\n",
    "    pipeline = results['logistic_enhanced']['pipeline']  # Use enhanced version as baseline\n",
    "else:\n",
    "    # Fallback: create original model structure\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n",
    "        ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(pipeline, 'models/hausa_sentiment/logreg_model.joblib')\n",
    "\n",
    "# Save label encoder\n",
    "if label_encoder:\n",
    "    joblib.dump(label_encoder, 'models/hausa_sentiment/label_encoder.joblib')\n",
    "\n",
    "# Enhanced: Save comprehensive results\n",
    "results_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_model': best_model,\n",
    "    'models_tested': len(results),\n",
    "    'results': {\n",
    "        name: {\n",
    "            'accuracy': float(result['accuracy']),\n",
    "            'f1_macro': float(result['f1_macro']),\n",
    "            'f1_weighted': float(result['f1_weighted'])\n",
    "        }\n",
    "        for name, result in results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('models/hausa_sentiment/training_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print('Models and results saved successfully!')\n",
    "print(f'Best model saved as: models/hausa_sentiment/best_model.joblib')\n",
    "print(f'Compatibility model saved as: models/hausa_sentiment/logreg_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572310c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
