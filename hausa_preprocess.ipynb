{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e75ef0e",
   "metadata": {},
   "source": [
    "# Hausa Sentiment Analysis: Data Preprocessing\n",
    "\n",
    "This notebook covers the preprocessing steps for Hausa sentiment analysis using the HausaBERTa transformer model. We will load the pre-split datasets, clean and preprocess the text, and prepare the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3cdd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8f9bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (14172, 2)\n",
      "Validation shape: (2677, 2)\n",
      "Test shape: (5303, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user Da kudin da Arewa babu wani abin azo aga...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@user Kaga wani Adu ar Banda💔😭 wai a haka Shi ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user Sai haquri fa yan madrid daman kunce cha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@user Hmmm yanzu kai kasan girman allah daxaka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user @user Wai gwamno nin Nigeria suna afa kw...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  @user Da kudin da Arewa babu wani abin azo aga...      2\n",
       "1  @user Kaga wani Adu ar Banda💔😭 wai a haka Shi ...      2\n",
       "2  @user Sai haquri fa yan madrid daman kunce cha...      2\n",
       "3  @user Hmmm yanzu kai kasan girman allah daxaka...      2\n",
       "4  @user @user Wai gwamno nin Nigeria suna afa kw...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Hausa Sentiment Dataset\n",
    "train_df = pd.read_csv('data/afrisenti_twitter_hausa_train.csv')\n",
    "val_df = pd.read_csv('data/afrisenti_twitter_hausa_validation.csv')\n",
    "test_df = pd.read_csv('data/afrisenti_twitter_hausa_test.csv')\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Validation shape:', val_df.shape)\n",
    "print('Test shape:', test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c7c0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample before cleaning:\n",
      "0    @user Da kudin da Arewa babu wani abin azo aga...\n",
      "1    @user Kaga wani Adu ar Banda💔😭 wai a haka Shi ...\n",
      "2    @user Sai haquri fa yan madrid daman kunce cha...\n",
      "3    @user Hmmm yanzu kai kasan girman allah daxaka...\n",
      "4    @user @user Wai gwamno nin Nigeria suna afa kw...\n",
      "Name: tweet, dtype: object\n",
      "\n",
      "Sample after cleaning:\n",
      "0    kudin arewa babu abin azo agani alummah allah ...\n",
      "1    kaga adu ar banda wai haka shugaban sojoji gas...\n",
      "2    haquri yan madrid daman kunce champion din ya ...\n",
      "3    hmm kasan girman allah daxakace mukuma allah k...\n",
      "4              wai gwamno nin nigeria suna afa kwayoyi\n",
      "Name: tweet_clean, dtype: object\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "1    4912\n",
      "0    4687\n",
      "2    4573\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation label distribution:\n",
      "label\n",
      "1    896\n",
      "2    894\n",
      "0    887\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label\n",
      "1    1789\n",
      "2    1759\n",
      "0    1755\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cleaned data exported for train, validation, and test sets.\n",
      "\n",
      "Sample after cleaning:\n",
      "0    kudin arewa babu abin azo agani alummah allah ...\n",
      "1    kaga adu ar banda wai haka shugaban sojoji gas...\n",
      "2    haquri yan madrid daman kunce champion din ya ...\n",
      "3    hmm kasan girman allah daxakace mukuma allah k...\n",
      "4              wai gwamno nin nigeria suna afa kwayoyi\n",
      "Name: tweet_clean, dtype: object\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "1    4912\n",
      "0    4687\n",
      "2    4573\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation label distribution:\n",
      "label\n",
      "1    896\n",
      "2    894\n",
      "0    887\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label\n",
      "1    1789\n",
      "2    1759\n",
      "0    1755\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cleaned data exported for train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Hausa Text Preprocessing (Comprehensive Class Version)\n",
    "import re\n",
    "import string\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "# class HausaTextPreprocessor:\n",
    "#     def __init__(self):\n",
    "#         self.hausa_stopwords = {\n",
    "#             'da', 'ne', 'ce', 'na', 'ta', 'shi', 'ita', 'su', 'ni', 'ka', 'ki', 'ku', 'mu', 'wa', 'zuwa', 'akan',\n",
    "#             'amma', 'ko', 'kuma', 'saboda', 'don', 'ba', 'bai', 'bata', 'baiwa', 'bayan', 'cikin', 'ga', 'ina',\n",
    "#             'yana', 'yake', 'yayi', 'yake', 'yanzu', 'wannan', 'wancan', 'wata', 'wani', 'wasu', 'duk', 'kowa',\n",
    "#             'me', 'mece', 'mene', 'wace', 'wane', 'wacece', 'wanene', 'inda', 'lokacin', 'idan', 'kamar', 'saboda',\n",
    "#             'daidai', 'kawai', 'har', 'sai', 'tun', 'daga', 'zuwa', 'kuma', 'ko', 'amma', 'saboda', 'idan', 'ko',\n",
    "#             'da', 'ba', 'ce', 'ne', 'shi', 'ta', 'su', 'ni', 'ka', 'ki', 'ku', 'mu', 'wa', 'zuwa', 'akan', 'ga',\n",
    "#             'cikin', 'bayan', 'lokacin', 'inda', 'yanzu', 'kamar', 'saboda', 'kawai', 'har', 'sai', 'tun', 'daga'\n",
    "#         }\n",
    "#         self.punctuation = set(string.punctuation)\n",
    "#         self.hausa_chars = set('abcdefghijklmnopqrstuvwxyz’ʼƙɗɓçäöüÀÁÂÃÈÉÊÌÍÎÒÓÔÕÙÚÛÇÑ')\n",
    "\n",
    "#     def clean_text(self, text: str) -> str:\n",
    "#         if pd.isna(text):\n",
    "#             return \"\"\n",
    "#         text = str(text).lower()\n",
    "#         text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "#         text = re.sub(r'\\S+@\\S+', '', text)\n",
    "#         text = re.sub(r'@[\\w_]+', '', text)\n",
    "#         text = re.sub(r'#[\\w_]+', '', text)\n",
    "#         text = re.sub(r'\\d+', '', text)\n",
    "#         # Remove emojis\n",
    "#         emoji_pattern = re.compile(\"[\"\n",
    "#             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#             u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#             u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#             u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#             u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "#             u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "#             \"]+\", flags=re.UNICODE)\n",
    "#         text = emoji_pattern.sub(r'', text)\n",
    "#         text = re.sub(r'\\s+', ' ', text)\n",
    "#         text = text.strip('\"\\'')\n",
    "#         return text.strip()\n",
    "\n",
    "#     def remove_punctuation(self, text: str) -> str:\n",
    "#         return ''.join(char for char in text if char not in self.punctuation)\n",
    "\n",
    "#     def keep_hausa_chars(self, text: str) -> str:\n",
    "#         return ''.join(char for char in text if char in self.hausa_chars or char.isspace())\n",
    "\n",
    "#     def tokenize(self, text: str) -> List[str]:\n",
    "#         return text.split()\n",
    "\n",
    "#     def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "#         return [token for token in tokens if token not in self.hausa_stopwords]\n",
    "\n",
    "#     def preprocess(self, text: str, remove_stopwords: bool = True, keep_only_hausa: bool = False) -> str:\n",
    "#         text = self.clean_text(text)\n",
    "#         text = self.remove_punctuation(text)\n",
    "#         if keep_only_hausa:\n",
    "#             text = self.keep_hausa_chars(text)\n",
    "#         tokens = self.tokenize(text)\n",
    "#         if remove_stopwords:\n",
    "#             tokens = self.remove_stopwords(tokens)\n",
    "#         return ' '.join(tokens)\n",
    "\n",
    "# Instantiate the preprocessor\n",
    "from hausa_preprocess import HausaTextPreprocessor\n",
    "preprocessor = HausaTextPreprocessor()\n",
    "\n",
    "# Show before/after samples for verification\n",
    "print('Sample before cleaning:')\n",
    "print(train_df['tweet'].head(5))\n",
    "\n",
    "# Apply robust Hausa preprocessing\n",
    "train_df['tweet_clean'] = train_df['tweet'].apply(lambda x: preprocessor.preprocess(x))\n",
    "val_df['tweet_clean'] = val_df['tweet'].apply(lambda x: preprocessor.preprocess(x))\n",
    "test_df['tweet_clean'] = test_df['tweet'].apply(lambda x: preprocessor.preprocess(x))\n",
    "\n",
    "print('\\nSample after cleaning:')\n",
    "print(train_df['tweet_clean'].head(5))\n",
    "\n",
    "# Show label distribution for verification\n",
    "if 'label' in train_df.columns:\n",
    "    print('\\nTrain label distribution:')\n",
    "    print(train_df['label'].value_counts())\n",
    "if 'label' in val_df.columns:\n",
    "    print('\\nValidation label distribution:')\n",
    "    print(val_df['label'].value_counts())\n",
    "if 'label' in test_df.columns:\n",
    "    print('\\nTest label distribution:')\n",
    "    print(test_df['label'].value_counts())\n",
    "\n",
    "# Export cleaned data for training and evaluation\n",
    "export_cols = ['tweet_clean', 'label']\n",
    "train_df[export_cols].to_csv('data/afrisenti_twitter_hausa_train_clean.csv', index=False)\n",
    "val_df[export_cols].to_csv('data/afrisenti_twitter_hausa_validation_clean.csv', index=False)\n",
    "test_df[export_cols].to_csv('data/afrisenti_twitter_hausa_test_clean.csv', index=False)\n",
    "print('\\nCleaned data exported for train, validation, and test sets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc7655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
